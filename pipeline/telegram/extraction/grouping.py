"""
–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á
"""
from typing import List, Dict, Any
from shared.ai.gpt5_client import get_openai_client
from pipeline.telegram.vectorization.embeddings import cosine_similarity_embedding


def find_similar_tasks(tasks: List[Dict[str, Any]], similarity_threshold: float = 0.85, client=None) -> Dict[int, List[int]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏ –∏—Å–ø–æ–ª—å–∑—É—è OpenAI embeddings.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å: {task_index: [—Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á]}
    
    Args:
        tasks: –°–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –≥—Ä—É–ø–ø–∞–º–∏ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á
    """
    if client is None:
        client = get_openai_client()
    
    if len(tasks) < 2:
        return {}
    
    print(f"\nüîó –ü–æ–∏—Å–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á —Å—Ä–µ–¥–∏ {len(tasks)} –∑–∞–¥–∞—á...")
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        task_texts = []
        for task in tasks:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
            text = f"{task.get('title', '')} {task.get('description', '')}"
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            text = ' '.join(text.split()[:50])  # –ü–µ—Ä–≤—ã–µ 50 —Å–ª–æ–≤
            task_texts.append(text)
        
        # –ü–æ–ª—É—á–∞–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
        print(f"   –ü–æ–ª—É—á–µ–Ω–∏–µ embeddings –¥–ª—è {len(task_texts)} –∑–∞–¥–∞—á...", end=" ", flush=True)
        embeddings_response = client.embeddings.create(
            model="text-embedding-3-small",
            input=task_texts
        )
        embeddings = [item.embedding for item in embeddings_response.data]
        print(f"‚úì –ø–æ–ª—É—á–µ–Ω–æ {len(embeddings)} embeddings")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤—Å–µ–º–∏ –ø–∞—Ä–∞–º–∏ –∑–∞–¥–∞—á
        similar_groups = {}
        processed = set()
        
        for i in range(len(tasks)):
            if i in processed:
                continue
            
            similar_to_i = [i]
            
            for j in range(i + 1, len(tasks)):
                if j in processed:
                    continue
                
                similarity = cosine_similarity_embedding(embeddings[i], embeddings[j])
                
                if similarity >= similarity_threshold:
                    similar_to_i.append(j)
                    processed.add(j)
            
            if len(similar_to_i) > 1:
                similar_groups[i] = similar_to_i
                processed.add(i)
        
        print(f"   –ù–∞–π–¥–µ–Ω–æ {len(similar_groups)} –≥—Ä—É–ø–ø —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á")
        return similar_groups
        
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á: {e}")
        return {}


def group_and_deduplicate_tasks(all_tasks: List[Dict[str, Any]], similarity_threshold: float = 0.85, client=None) -> Dict[str, Any]:
    """
    –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á–∏ –∏ –Ω–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–¥–∞—á–∞–º–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞—Ç–æ–≤.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π.
    
    Args:
        all_tasks: –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        similarity_threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏ –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    """
    if client is None:
        client = get_openai_client()
    
    print(f"\nüìä –ê–Ω–∞–ª–∏–∑ {len(all_tasks)} –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á...")
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–¥–∞—á–∏
    similar_groups = find_similar_tasks(all_tasks, similarity_threshold, client)
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    result = {
        "total_tasks": len(all_tasks),
        "unique_tasks": [],
        "duplicate_groups": [],
        "tasks_by_chat": {},
        "tasks_by_status": {}
    }
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —á–∞—Ç–∞–º (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∏ —Å—Ç–∞—Ä—É—é –∏ –Ω–æ–≤—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É)
    for task in all_tasks:
        # –ù–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: chats - –º–∞—Å—Å–∏–≤, —Å—Ç–∞—Ä–∞—è: chat_name - —Å—Ç—Ä–æ–∫–∞
        chats = task.get('chats', [])
        if not chats and task.get('chat_name'):
            chats = [task.get('chat_name')]
        if not chats:
            chats = ['–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ']
        
        for chat in chats:
            if chat not in result["tasks_by_chat"]:
                result["tasks_by_chat"][chat] = []
            result["tasks_by_chat"][chat].append(task)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Å—Ç–∞—Ç—É—Å—É
    for task in all_tasks:
        status = task.get('status', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
        if status not in result["tasks_by_status"]:
            result["tasks_by_status"][status] = []
        result["tasks_by_status"][status].append(task)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≥—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–¥–∞—á
    processed_indices = set()
    for main_idx, similar_indices in similar_groups.items():
        if main_idx in processed_indices:
            continue
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä—É–ø–ø—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —á–∞—Ç—ã –∏–∑ –∑–∞–¥–∞—á (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –æ–±–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
        all_chats_in_group = set()
        for idx in similar_indices:
            task = all_tasks[idx]
            chats = task.get('chats', [])
            if not chats and task.get('chat_name'):
                chats = [task.get('chat_name')]
            all_chats_in_group.update(chats)
        
        group = {
            "main_task": all_tasks[main_idx],
            "related_tasks": [all_tasks[idx] for idx in similar_indices if idx != main_idx],
            "total_occurrences": len(similar_indices),
            "chats": list(all_chats_in_group) if all_chats_in_group else ['–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ']
        }
        
        result["duplicate_groups"].append(group)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∑–∞–¥–∞—á—É –≤ unique_tasks
        main_task = all_tasks[main_idx].copy()
        main_task["related_chats"] = group["chats"]
        main_task["total_mentions"] = len(similar_indices)
        result["unique_tasks"].append(main_task)
        
        # –ü–æ–º–µ—á–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –≤ –≥—Ä—É–ø–ø–µ –∫–∞–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
        processed_indices.update(similar_indices)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –±—ã–ª–∏ —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω—ã
    for i, task in enumerate(all_tasks):
        if i not in processed_indices:
            result["unique_tasks"].append(task)
    
    print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á: {len(result['unique_tasks'])}")
    print(f"   –ì—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(result['duplicate_groups'])}")
    
    return result

