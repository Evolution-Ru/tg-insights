"""
–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∂–∞—Ç–∏—è –¥–∏–∞–ª–æ–≥–æ–≤ —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ —Å–∫–æ–ª—å–∑—è—â–∏–º –æ–∫–Ω–æ–º
"""
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Any
from .chunking import split_thread_by_dates
from .compressor import compress_chunk
from .batch_processor import process_chunks_via_batch_with_dates
from .sliding_window import apply_sliding_window
from shared.ai.gpt5_client import get_openai_client


def compress_thread_with_smart_model(
    thread_text: str, 
    max_chunk_size: int = 10000,
    output_dir: Path = None,
    client=None
) -> str:
    """
    –°–∂–∏–º–∞–µ—Ç –¥–∏–∞–ª–æ–≥ –¥–æ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É—è —É–º–Ω—É—é –º–æ–¥–µ–ª—å (gpt-5).
    –†–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –¥–Ω—è–º –∏ —Å–∏–º–≤–æ–ª–∞–º - –∫–∞–∂–¥—ã–π –±–ª–æ–∫ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –∑–∞–∫–æ–Ω—á–µ–Ω–Ω—ã–º –¥–Ω–µ–º.
    
    –í–ê–ñ–ù–û: –†–∞–∑–±–∏–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ - –≥—Ä–∞–Ω–∏—Ü—ã —á–∞—Å—Ç–µ–π –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–æ –¥–∞—Ç–∞–º, 
    –ø–æ—ç—Ç–æ–º—É –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–æ–Ω–µ—Ü –Ω–µ –º–µ–Ω—è–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —á–∞—Å—Ç–µ–π.
    
    –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ —á–∞—Å—Ç–∏ —Å –Ω–æ–≤—ã–º–∏ –¥–∞—Ç–∞–º–∏.
    –°–∫–æ–ª—å–∑—è—â–∞—è –≤—ã–∂–∏–º–∫–∞: –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 —á–∞—Å—Ç–∏ + –ø—Ä–µ–¥—ã–¥—É—â–∞—è –≤—ã–∂–∏–º–∫–∞.
    
    Args:
        thread_text: –¢–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ –¥–ª—è —Å–∂–∞—Ç–∏—è
        max_chunk_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–µ—Ñ–æ–ª—Ç–Ω–∞—è)
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∂–∞—Ç–∞—è –≤—ã–∂–∏–º–∫–∞ –¥–∏–∞–ª–æ–≥–∞
    """
    if client is None:
        client = get_openai_client()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if output_dir is None:
        script_dir = Path(__file__).resolve().parent
        project_root = script_dir.parent.parent.parent  # tg-analyz/
        output_dir = project_root / "results" / "farma" / "compressed_parts"
    
    cache_dir = output_dir / "cache"
    output_dir.mkdir(parents=True, exist_ok=True)
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    total_length = len(thread_text)
    print(f"\nüß† –°–∂–∞—Ç–∏–µ –¥–∏–∞–ª–æ–≥–∞ ({total_length} —Å–∏–º–≤–æ–ª–æ–≤)...")
    
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ–±–æ–ª—å—à–æ–π, —Å–∂–∏–º–∞–µ–º —Ü–µ–ª–∏–∫–æ–º
    if total_length <= max_chunk_size:
        return compress_chunk(thread_text, client)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –¥–∞—Ç–∞–º
    print(f"   –¢–µ–∫—Å—Ç –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞—é –Ω–∞ —á–∞—Å—Ç–∏ –ø–æ –¥–Ω—è–º (–º–∞–∫—Å. {max_chunk_size} —Å–∏–º–≤–æ–ª–æ–≤)...")
    chunks_meta = split_thread_by_dates(thread_text, max_chunk_size)
    
    print(f"   –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks_meta)} —á–∞—Å—Ç–µ–π")
    for i, meta in enumerate(chunks_meta, 1):
        date_info = f" ({meta['first_date']} - {meta['last_date']})" if meta['first_date'] else ""
        print(f"      –ß–∞—Å—Ç—å {i}: {len(meta['chunk'])} —Å–∏–º–≤–æ–ª–æ–≤{date_info}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç—è—Ö
    parts_metadata_file = output_dir / "parts_metadata.json"
    processed_parts = {}  # {hash: {dates, compressed_text_hash}}
    if parts_metadata_file.exists():
        with open(parts_metadata_file, "r", encoding="utf-8") as f:
            processed_parts = json.load(f)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —á–∞—Å—Ç–∏ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ)
    chunks_to_process = []  # [(index, chunk_meta, hash)]
    cached_results = {}  # {hash: compressed_text}
    new_dates = set()  # –í—Å–µ –¥–∞—Ç—ã –∏–∑ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
    
    # –°–Ω–∞—á–∞–ª–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –¥–∞—Ç—ã —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
    processed_dates = set()
    for part_hash, part_info in processed_parts.items():
        if 'date_range' in part_info:
            processed_dates.update(part_info['date_range'])
    
    print(f"\n   –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–µ—à–∞ –¥–ª—è {len(chunks_meta)} —á–∞—Å—Ç–µ–π...")
    print(f"   –£–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∞—Ç: {len(processed_dates)}")
    
    for i, meta in enumerate(chunks_meta, 1):
        chunk = meta['chunk']
        chunk_hash = hashlib.sha256(chunk.encode('utf-8')).hexdigest()[:16]
        cache_file = cache_dir / f"{chunk_hash}.txt"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —á–∞—Å—Ç—å –Ω–æ–≤—ã–µ –¥–∞—Ç—ã
        part_dates = set(meta['date_range']) if meta['date_range'] else set()
        has_new_dates = bool(part_dates - processed_dates)
        
        if cache_file.exists() and not has_new_dates:
            # –ß–∞—Å—Ç—å —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–æ–≤—ã—Ö –¥–∞—Ç
            with open(cache_file, "r", encoding="utf-8") as f:
                cached_results[chunk_hash] = f.read()
            print(f"   ‚úì –ß–∞—Å—Ç—å {i}: –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ ({chunk_hash})")
        else:
            # –ß–∞—Å—Ç—å –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (–Ω–æ–≤–∞—è –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–æ–≤—ã–µ –¥–∞—Ç—ã)
            chunks_to_process.append((i, meta, chunk_hash))
            new_dates.update(part_dates)
            if has_new_dates:
                print(f"   ‚è≥ –ß–∞—Å—Ç—å {i}: —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–æ–≤—ã–µ –¥–∞—Ç—ã {sorted(part_dates - processed_dates)} ({chunk_hash})")
            else:
                print(f"   ‚è≥ –ß–∞—Å—Ç—å {i}: –Ω—É–∂–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ ({chunk_hash})")
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —á–∞—Å—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞—Ç—á
    if chunks_to_process:
        print(f"\n   üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(chunks_to_process)} —á–∞—Å—Ç–µ–π —á–µ—Ä–µ–∑ Batch API...")
        batch_results = process_chunks_via_batch_with_dates(
            chunks_to_process, 
            cache_dir, 
            parts_metadata_file,
            client
        )
        cached_results.update(batch_results)
    
    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    compressed_chunks = []
    for i, meta in enumerate(chunks_meta, 1):
        chunk_hash = hashlib.sha256(meta['chunk'].encode('utf-8')).hexdigest()[:16]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫–µ—à–µ
        if chunk_hash not in cached_results:
            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –∫–µ—à–∞ (–≤–æ–∑–º–æ–∂–Ω–æ —Ñ–∞–π–ª –±—ã–ª —Å–æ–∑–¥–∞–Ω, –Ω–æ –Ω–µ –ø–æ–ø–∞–ª –≤ —Å–ª–æ–≤–∞—Ä—å)
            cache_file = cache_dir / f"{chunk_hash}.txt"
            if cache_file.exists():
                print(f"   ‚ö† –ß–∞—Å—Ç—å {i}: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–∞–º—è—Ç–∏, –∑–∞–≥—Ä—É–∂–∞—é –∏–∑ –∫–µ—à–∞...")
                with open(cache_file, "r", encoding="utf-8") as f:
                    cached_results[chunk_hash] = f.read()
            else:
                # Fallback: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é –µ—Å–ª–∏ –±–∞—Ç—á –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                print(f"   ‚ö† –ß–∞—Å—Ç—å {i}: —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –Ω–∞–ø—Ä—è–º—É—é...")
                try:
                    compressed = compress_chunk(meta['chunk'], client)
                    cached_results[chunk_hash] = compressed
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                    with open(cache_file, "w", encoding="utf-8") as f:
                        f.write(compressed)
                    print(f"   ‚úì –ß–∞—Å—Ç—å {i}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –Ω–∞–ø—Ä—è–º—É—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –∫–µ—à")
                except Exception as e:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —á–∞—Å—Ç–∏ {i}: {e}")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –∫–∞–∫ fallback
                    cached_results[chunk_hash] = meta['chunk']
                    print(f"   ‚ö† –ò—Å–ø–æ–ª—å–∑—É—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è —á–∞—Å—Ç–∏ {i}")
        
        # –¢–µ–ø–µ—Ä—å –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ cached_results
        compressed = cached_results[chunk_hash]
        compressed_chunks.append(compressed)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é —Å–∂–∞—Ç—É—é —á–∞—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω–æ (–¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞)
        part_file = output_dir / f"part_{i:02d}_compressed.txt"
        with open(part_file, "w", encoding="utf-8") as f:
            f.write(compressed)
        print(f"   üíæ –ß–∞—Å—Ç—å {i} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {part_file} ({len(compressed)} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # –°–∫–æ–ª—å–∑—è—â–∞—è –≤—ã–∂–∏–º–∫–∞: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –≤—ã–∂–∏–º–∫–∏ + –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –≤—ã–∂–∏–º–∫–∞
    final_summary = apply_sliding_window(compressed_chunks, output_dir, cache_dir, client)
    
    return final_summary

