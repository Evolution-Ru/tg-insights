"""
Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ OpenAI Batch API
"""
import json
import time
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from shared.ai.gpt5_client import get_openai_client, parse_gpt5_response


def check_active_batches(metadata_file: Path, client=None) -> List[Dict]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π (validating, in_progress).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π.
    
    Args:
        metadata_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –±–∞—Ç—á–µ–π
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–∞—Ö
    """
    if client is None:
        client = get_openai_client()
    
    active_batches = []
    
    if not metadata_file.exists():
        return active_batches
    
    try:
        with open(metadata_file, "r", encoding="utf-8") as f:
            batch_metadata_list = json.load(f)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –±–∞—Ç—á–µ–π (—á—Ç–æ–±—ã –Ω–µ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –≤—Å–µ —Å—Ç–∞—Ä—ã–µ)
        recent_batches = batch_metadata_list[-10:] if len(batch_metadata_list) > 10 else batch_metadata_list
        
        for batch_meta in recent_batches:
            batch_id = batch_meta.get("batch_id")
            if not batch_id:
                continue
            
            try:
                batch_status = client.batches.retrieve(batch_id)
                status = batch_status.status
                
                if status in ["validating", "in_progress"]:
                    active_batches.append({
                        "batch_id": batch_id,
                        "status": status,
                        "created_at": batch_meta.get("created_at_iso", "unknown"),
                        "total_chunks": batch_meta.get("total_chunks", 0)
                    })
            except Exception as e:
                # –ë–∞—Ç—á –º–æ–∂–µ—Ç –±—ã—Ç—å —É–¥–∞–ª–µ–Ω –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
                
    except Exception as e:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
        pass
    
    return active_batches


def check_duplicate_batches(metadata_file: Path, chunk_hashes: List[str], client=None) -> Optional[Dict]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –±–∞—Ç—á–µ–π —Å —Ç–µ–º–∏ –∂–µ —Ö–µ—à–∞–º–∏ —á–∞–Ω–∫–æ–≤ (–¥—É–±–ª–∏–∫–∞—Ç—ã).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω–æ–º –¥—É–±–ª–∏–∫–∞—Ç–µ –∏–ª–∏ None.
    
    Args:
        metadata_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –±–∞—Ç—á–µ–π
        chunk_hashes: –°–ø–∏—Å–æ–∫ —Ö–µ—à–µ–π —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥—É–±–ª–∏–∫–∞—Ç–µ –∏–ª–∏ None
    """
    if client is None:
        client = get_openai_client()
    
    if not metadata_file.exists():
        return None
    
    try:
        with open(metadata_file, "r", encoding="utf-8") as f:
            batch_metadata_list = json.load(f)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –±–∞—Ç—á–µ–π
        recent_batches = batch_metadata_list[-10:] if len(batch_metadata_list) > 10 else batch_metadata_list
        
        # –°–æ–∑–¥–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ —Ö–µ—à–µ–π –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        new_hashes_set = set(chunk_hashes)
        
        for batch_meta in reversed(recent_batches):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å –∫–æ–Ω—Ü–∞ (–Ω–æ–≤—ã–µ —Å–Ω–∞—á–∞–ª–∞)
            batch_id = batch_meta.get("batch_id")
            if not batch_id:
                continue
            
            # –ü–æ–ª—É—á–∞–µ–º —Ö–µ—à–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –±–∞—Ç—á–∞
            batch_chunks = batch_meta.get("chunks", [])
            batch_hashes = [chunk.get("chunk_hash") for chunk in batch_chunks if chunk.get("chunk_hash")]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–≤–ø–∞–¥–∞—é—Ç –ª–∏ —Ö–µ—à–∏
            if set(batch_hashes) == new_hashes_set:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å –±–∞—Ç—á–∞ —á–µ—Ä–µ–∑ API
                try:
                    batch_status = client.batches.retrieve(batch_id)
                    status = batch_status.status
                    
                    return {
                        "batch_id": batch_id,
                        "status": status,
                        "created_at": batch_meta.get("created_at_iso", "unknown"),
                        "total_chunks": len(batch_hashes)
                    }
                except Exception:
                    # –ë–∞—Ç—á –º–æ–∂–µ—Ç –±—ã—Ç—å —É–¥–∞–ª–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    continue
                    
    except Exception as e:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
        pass
    
    return None


def process_chunks_via_batch(
    chunks_to_process: List[Tuple[int, str, str]], 
    cache_dir: Path,
    client=None
) -> Dict[str, str]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–∏ —á–µ—Ä–µ–∑ Batch API –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¢–û–õ–¨–ö–û Batch API (–±–µ–∑ fallback –Ω–∞ –æ–±—ã—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {hash: compressed_text}
    
    Args:
        chunks_to_process: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (index, chunk_text, chunk_hash)
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫–µ—à–∞
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å {hash: compressed_text}
    """
    if client is None:
        client = get_openai_client()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π –∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º –Ω–æ–≤–æ–≥–æ
    metadata_file = cache_dir.parent / "batch_metadata.json"
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ö–µ—à–∏ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    chunk_hashes = [chunk_hash for _, _, chunk_hash in chunks_to_process]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ö–µ—à–∞–º
    duplicate = check_duplicate_batches(metadata_file, chunk_hashes, client)
    if duplicate:
        if duplicate["status"] == "completed":
            print(f"\n      ‚úÖ –ù–∞–π–¥–µ–Ω –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–π –±–∞—Ç—á-–¥—É–±–ª–∏–∫–∞—Ç: {duplicate['batch_id']}")
            print(f"      üí° –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –±–∞—Ç—á–∞ –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ.\n")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –±–∞—Ç—á–∞
            return {}
        else:
            print(f"\n      ‚ö†Ô∏è  –ù–∞–π–¥–µ–Ω –∞–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á-–¥—É–±–ª–∏–∫–∞—Ç: {duplicate['batch_id']} ({duplicate['status']})")
            print(f"      üí° –î–æ–∂–∏–¥–∞–µ–º—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –±–∞—Ç—á–∞ –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ.\n")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å - –¥–æ–∂–∏–¥–∞–µ–º—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –±–∞—Ç—á–∞
            return {}
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –±–∞—Ç—á–∏ (–¥–ª—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è)
    active_batches = check_active_batches(metadata_file, client)
    if active_batches:
        print(f"\n      ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(active_batches)} –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π:")
        for ab in active_batches:
            print(f"         - {ab['batch_id']}: {ab['status']} ({ab['total_chunks']} —á–∞—Å—Ç–µ–π, —Å–æ–∑–¥–∞–Ω {ab['created_at']})")
        print(f"      üí° –°–æ–∑–¥–∞—é –Ω–æ–≤—ã–π –±–∞—Ç—á, –Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ–∂–¥–∞—Ç—å—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω—ã—Ö.\n")
    
    print(f"      üìù –°–æ–∑–¥–∞–Ω–∏–µ JSONL —Ñ–∞–π–ª–∞ –¥–ª—è –±–∞—Ç—á–∞...")
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π JSONL —Ñ–∞–π–ª
    temp_jsonl = tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False, encoding='utf-8')
    
    system_prompt = "–¢—ã –ø–æ–º–æ–≥–∞–µ—à—å —Å–∂–∏–º–∞—Ç—å –ø–µ—Ä–µ–ø–∏—Å–∫–∏ –¥–æ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤."
    
    for idx, chunk, chunk_hash in chunks_to_process:
        user_prompt = f"""–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –ø–µ—Ä–µ–ø–∏—Å–∫—É –ø–æ –ø—Ä–æ–µ–∫—Ç—É –§–∞—Ä–º–∞+. 

–°–æ–∂–º–∏ –¥–∏–∞–ª–æ–≥ –¥–æ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤:
- –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã –æ–±—Å—É–∂–¥–µ–Ω–∏–π
- –ü—Ä–∏–Ω—è—Ç—ã–µ —Ä–µ—à–µ–Ω–∏—è
- –ü–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞
- –î–µ–¥–ª–∞–π–Ω—ã –∏ —Å—Ä–æ–∫–∏
- –í–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –ø–æ –ø—Ä–æ–µ–∫—Ç—É

–°–æ—Ö—Ä–∞–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏–∞–ª–æ–≥–∞ (—á–∞—Ç—ã, —É—á–∞—Å—Ç–Ω–∏–∫–∏, –¥–∞—Ç—ã), –Ω–æ —É–¥–∞–ª–∏:
- –ü–æ–≤—Ç–æ—Ä—ã –∏ —É—Ç–æ—á–Ω–µ–Ω–∏—è
- –ú–µ–ª–∫–∏–µ –¥–µ—Ç–∞–ª–∏
- –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏ –ø—Ä–æ—â–∞–Ω–∏—è
- –ù–µ—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏

–í–µ—Ä–Ω–∏ —Å–∂–∞—Ç—ã–π –¥–∏–∞–ª–æ–≥, —Å–æ—Ö—Ä–∞–Ω—è—è –≤–∞–∂–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∑–∞–¥–∞—á –∏ —Ä–µ—à–µ–Ω–∏–π.

–ò—Å—Ö–æ–¥–Ω—ã–π –¥–∏–∞–ª–æ–≥:
{chunk}"""
        
        # –§–æ—Ä–º–∞—Ç –¥–ª—è responses API –≤ Batch: input –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º —Å–ª–æ–≤–∞—Ä–µ–π —Å role/content
        request_data = {
            "custom_id": f"chunk_{idx}_{chunk_hash}",
            "method": "POST",
            "url": "/v1/responses",
            "body": {
                "model": "gpt-5",
                "input": [
                    {
                        "role": "system",
                        "content": system_prompt
                    },
                    {
                        "role": "user",
                        "content": user_prompt
                    }
                ],
                "reasoning": {"effort": "low"}
            }
        }
        
        temp_jsonl.write(json.dumps(request_data, ensure_ascii=False) + '\n')
    
    temp_jsonl.close()
    jsonl_path = Path(temp_jsonl.name)
    
    print(f"      üì§ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ –≤ OpenAI...")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
    with open(jsonl_path, 'rb') as f:
        uploaded_file = client.files.create(
            file=f,
            purpose="batch"
        )
    print(f"      ‚úì –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {uploaded_file.id}")
    
    # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á
    print(f"      üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –±–∞—Ç—á–∞...")
    batch = client.batches.create(
        input_file_id=uploaded_file.id,
        endpoint="/v1/responses",
        completion_window="24h"
    )
    batch_id = batch.id
    print(f"      ‚úì –ë–∞—Ç—á —Å–æ–∑–¥–∞–Ω: {batch_id}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞
    batch_metadata = {
        "batch_id": batch_id,
        "created_at": time.time(),
        "created_at_iso": time.strftime("%Y-%m-%d %H:%M:%S"),
        "chunks": [
            {
                "index": idx,
                "chunk_hash": chunk_hash,
                "custom_id": f"chunk_{idx}_{chunk_hash}",
                "chunk_size": len(chunk)
            }
            for idx, chunk, chunk_hash in chunks_to_process
        ],
        "total_chunks": len(chunks_to_process),
        "status": "created",
        "input_file_id": uploaded_file.id
    }
    
    # metadata_file —É–∂–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤—ã—à–µ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –±–∞—Ç—á–µ–π
    batch_metadata_list = []
    if metadata_file.exists():
        with open(metadata_file, "r", encoding="utf-8") as f:
            batch_metadata_list = json.load(f)
    
    batch_metadata_list.append(batch_metadata)
    with open(metadata_file, "w", encoding="utf-8") as f:
        json.dump(batch_metadata_list, f, ensure_ascii=False, indent=2)
    
    print(f"      üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {metadata_file}")
    print(f"      ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–∞—Ç—á–∞ (—ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
    
    # –î–æ–∂–∏–¥–∞–µ–º—Å—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –±–∞—Ç—á–∞
    max_wait_time = 3600  # –ú–∞–∫—Å–∏–º—É–º 1 —á–∞—Å
    start_time = time.time()
    poll_interval = 10  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 10 —Å–µ–∫—É–Ω–¥
    
    while True:
        elapsed = time.time() - start_time
        if elapsed > max_wait_time:
            raise Exception(f"–ë–∞—Ç—á –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –∑–∞ {max_wait_time} —Å–µ–∫—É–Ω–¥")
        
        batch_status = client.batches.retrieve(batch_id)
        status = batch_status.status
        
        if status == "completed":
            print(f"      ‚úì –ë–∞—Ç—á –∑–∞–≤–µ—Ä—à–µ–Ω!")
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞
            batch_metadata["status"] = "completed"
            batch_metadata["completed_at"] = time.time()
            batch_metadata["completed_at_iso"] = time.strftime("%Y-%m-%d %H:%M:%S")
            batch_metadata["processing_time_seconds"] = elapsed
            batch_metadata["output_file_id"] = batch_status.output_file_id if hasattr(batch_status, 'output_file_id') else None
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–∫–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if metadata_file.exists():
                with open(metadata_file, "r", encoding="utf-8") as f:
                    batch_metadata_list = json.load(f)
                # –ù–∞—Ö–æ–¥–∏–º –Ω–∞—à –±–∞—Ç—á –∏ –æ–±–Ω–æ–≤–ª—è–µ–º
                for bm in batch_metadata_list:
                    if bm.get("batch_id") == batch_id:
                        bm.update(batch_metadata)
                        break
                with open(metadata_file, "w", encoding="utf-8") as f:
                    json.dump(batch_metadata_list, f, ensure_ascii=False, indent=2)
            break
        elif status == "failed":
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            batch_metadata["status"] = "failed"
            batch_metadata["failed_at"] = time.time()
            batch_metadata["failed_at_iso"] = time.strftime("%Y-%m-%d %H:%M:%S")
            if metadata_file.exists():
                with open(metadata_file, "r", encoding="utf-8") as f:
                    batch_metadata_list = json.load(f)
                for bm in batch_metadata_list:
                    if bm.get("batch_id") == batch_id:
                        bm.update(batch_metadata)
                        break
                with open(metadata_file, "w", encoding="utf-8") as f:
                    json.dump(batch_metadata_list, f, ensure_ascii=False, indent=2)
            raise Exception(f"–ë–∞—Ç—á –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —Å –æ—à–∏–±–∫–æ–π: {batch_status}")
        elif status in ["cancelled", "expired"]:
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –æ—Ç–º–µ–Ω–µ/–∏—Å—Ç–µ—á–µ–Ω–∏–∏
            batch_metadata["status"] = status
            batch_metadata[f"{status}_at"] = time.time()
            batch_metadata[f"{status}_at_iso"] = time.strftime("%Y-%m-%d %H:%M:%S")
            if metadata_file.exists():
                with open(metadata_file, "r", encoding="utf-8") as f:
                    batch_metadata_list = json.load(f)
                for bm in batch_metadata_list:
                    if bm.get("batch_id") == batch_id:
                        bm.update(batch_metadata)
                        break
                with open(metadata_file, "w", encoding="utf-8") as f:
                    json.dump(batch_metadata_list, f, ensure_ascii=False, indent=2)
            raise Exception(f"–ë–∞—Ç—á –±—ã–ª –æ—Ç–º–µ–Ω–µ–Ω –∏–ª–∏ –∏—Å—Ç–µ–∫: {status}")
        
        print(f"      ‚Üí –°—Ç–∞—Ç—É—Å: {status} (–ø—Ä–æ—à–ª–æ {elapsed:.0f} —Å–µ–∫)...", flush=True)
        time.sleep(poll_interval)
    
    # –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"      üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    output_file_id = batch_status.output_file_id
    if not output_file_id:
        raise Exception("–ù–µ—Ç output_file_id –≤ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–º –±–∞—Ç—á–µ")
    
    output_file = client.files.content(output_file_id)
    output_content = output_file.read().decode('utf-8')
    
    # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"      üîç –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    results = {}
    for line in output_content.strip().split('\n'):
        if not line:
            continue
        result_data = json.loads(line)
        custom_id = result_data.get('custom_id', '')
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º compressed —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
        response_body = result_data.get('response', {}).get('body', {})
        compressed = ""
        
        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç responses API
        if 'output_text' in response_body:
            compressed = response_body['output_text']
        elif 'output' in response_body:
            output = response_body['output']
            if isinstance(output, str):
                compressed = output
            elif isinstance(output, list):
                chunks = []
                for item in output:
                    if isinstance(item, dict):
                        if 'text' in item:
                            chunks.append(item['text'])
                        elif 'content' in item:
                            for c in item.get('content', []):
                                if isinstance(c, dict) and 'text' in c:
                                    chunks.append(c['text'])
                    elif hasattr(item, 'text'):
                        chunks.append(item.text)
                compressed = '\n'.join(chunks)
        
        if not compressed:
            # Fallback: –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ error –∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ –ø–æ–ª—è
            if 'error' in result_data.get('response', {}):
                error_info = result_data['response']['error']
                print(f"      ‚ö† –û—à–∏–±–∫–∞ –¥–ª—è {custom_id}: {error_info}")
                continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º hash –∏–∑ custom_id
        if custom_id.startswith('chunk_'):
            parts = custom_id.split('_')
            if len(parts) >= 3:
                chunk_hash = parts[2]
                results[chunk_hash] = compressed.strip()
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                cache_file = cache_dir / f"{chunk_hash}.txt"
                with open(cache_file, "w", encoding="utf-8") as f:
                    f.write(compressed.strip())
                print(f"      ‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω {custom_id}: {len(compressed)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    print(f"      ‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)}/{len(chunks_to_process)} —á–∞—Å—Ç–µ–π")
    
    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    try:
        jsonl_path.unlink()
    except:
        pass
    
    return results


def process_chunks_via_batch_with_dates(
    chunks_to_process: List[Tuple[int, Dict, str]], 
    cache_dir: Path,
    parts_metadata_file: Path,
    client=None
) -> Dict[str, str]:
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–∏ —á–µ—Ä–µ–∑ Batch API —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ –¥–∞—Ç–∞—Ö.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å {hash: compressed_text}
    
    Args:
        chunks_to_process: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (index, chunk_meta, hash) –≥–¥–µ chunk_meta - —Å–ª–æ–≤–∞—Ä—å —Å 'chunk', 'date_range' –∏ —Ç.–¥.
        cache_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∫–µ—à–∞
        parts_metadata_file: –§–∞–π–ª –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–µ–π
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å {hash: compressed_text}
    """
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è process_chunks_via_batch: (idx, chunk_text, hash)
    chunks_for_batch = [(idx, meta['chunk'], chunk_hash) for idx, meta, chunk_hash in chunks_to_process]
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ñ—É–Ω–∫—Ü–∏—é
    results = process_chunks_via_batch(chunks_for_batch, cache_dir, client)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞—Å—Ç–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –¥–∞—Ç–∞—Ö
    processed_parts = {}
    if parts_metadata_file.exists():
        with open(parts_metadata_file, "r", encoding="utf-8") as f:
            processed_parts = json.load(f)
    
    for idx, meta, chunk_hash in chunks_to_process:
        processed_parts[chunk_hash] = {
            'index': idx,
            'first_date': meta.get('first_date'),
            'last_date': meta.get('last_date'),
            'date_range': meta.get('date_range', []),
            'chunk_size': len(meta['chunk']),
            'compressed_size': len(results.get(chunk_hash, ''))
        }
    
    with open(parts_metadata_file, "w", encoding="utf-8") as f:
        json.dump(processed_parts, f, ensure_ascii=False, indent=2)
    
    return results

