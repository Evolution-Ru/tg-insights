#!/usr/bin/env python3
"""
–†—É—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç GPT-4o –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤.
"""

import os
import sys
import json
import sqlite3
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
from openai import OpenAI


def load_account_env(account_name: str) -> None:
    """Load environment from account .env"""
    current_dir = Path(__file__).resolve().parent
    env_path = current_dir.parent.parent / "accounts" / account_name / ".env"
    
    if not env_path.exists():
        raise SystemExit(f"Environment file not found: {env_path}")
    
    load_dotenv(env_path)


def get_sample_contexts(db_path: Path, limit: int = 10) -> List[Dict[str, Any]]:
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –∏–∑ –ë–î"""
    conn = sqlite3.connect(str(db_path))
    
    query = """
        SELECT 
            id,
            dialog_id,
            message_date,
            context_text
        FROM dialog_contexts
        WHERE LENGTH(context_text) > 100
        ORDER BY RANDOM()
        LIMIT ?
    """
    
    cursor = conn.execute(query, (limit,))
    contexts = []
    
    for row in cursor.fetchall():
        contexts.append({
            "id": row[0],
            "dialog_id": row[1],
            "message_date": row[2],
            "context_text": row[3]
        })
    
    conn.close()
    return contexts


def screening_prompt(context_text: str) -> str:
    """–ü—Ä–æ–º–ø—Ç –¥–ª—è —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–£—Ä–æ–≤–µ–Ω—å 1)"""
    return f"""–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ –∏–∑ Telegram –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∞–∂–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏.

–¢–ò–ü–´ –ê–†–¢–ï–§–ê–ö–¢–û–í:
1. commitment - –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ (–∫—Ç–æ-—Ç–æ –æ–±–µ—â–∞–µ—Ç —á—Ç–æ-—Ç–æ —Å–¥–µ–ª–∞—Ç—å –∫ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É —Å—Ä–æ–∫—É)
2. request - –∑–∞–ø—Ä–æ—Å (–∫—Ç–æ-—Ç–æ –ø—Ä–æ—Å–∏—Ç –ø–æ–º–æ—â–∏, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –¥–µ–π—Å—Ç–≤–∏—è)
3. decision - —Ä–µ—à–µ–Ω–∏–µ (–ø—Ä–∏–Ω—è—Ç–æ –≤–∞–∂–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –æ –ø—Ä–æ–µ–∫—Ç–µ, –ø—Ä–æ–¥—É–∫—Ç–µ, –ø—Ä–æ—Ü–µ—Å—Å–µ)
4. deadline - –¥–µ–¥–ª–∞–π–Ω (—É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –¥–∞—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏/–ø—Ä–æ–µ–∫—Ç–∞)
5. agreement - –¥–æ–≥–æ–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç—å (—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω—ã –≤—Å—Ç—Ä–µ—á–∏, —É—Å–ª–æ–≤–∏—è, –ø–ª–∞–Ω—ã)

–ö–û–ù–¢–ï–ö–°–¢ –î–ò–ê–õ–û–ì–ê:
```
{context_text}
```

–ó–ê–î–ê–ß–ê:
–û–ø—Ä–µ–¥–µ–ª–∏, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —ç—Ç–æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –∏–∑ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤—ã—à–µ.

–û–¢–í–ï–¢ (—Å—Ç—Ä–æ–≥–æ JSON):
{{
  "has_artifacts": true/false,
  "artifact_types": ["commitment", "request", ...],
  "confidence": 0.0-1.0,
  "reasoning": "–∫—Ä–∞—Ç–∫–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º"
}}

–ü–†–ê–í–ò–õ–ê:
- –ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω –Ω–∞ 70%+ ‚Üí has_artifacts: false
- –£–∫–∞–∑—ã–≤–∞–π –í–°–ï –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
- confidence –æ—Ç—Ä–∞–∂–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (0.0 = —Å–æ–≤—Å–µ–º –Ω–µ —É–≤–µ—Ä–µ–Ω, 1.0 = –∞–±—Å–æ–ª—é—Ç–Ω–æ —É–≤–µ—Ä–µ–Ω)"""


def test_screening(client: OpenAI, contexts: List[Dict[str, Any]], model: str = "gpt-4o"):
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å–∫—Ä–∏–Ω–∏–Ω–≥–∞ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö"""
    print(f"\n{'='*80}")
    print(f"üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –°–ö–†–ò–ù–ò–ù–ì–ê (–£—Ä–æ–≤–µ–Ω—å 1)")
    print(f"üìä –ú–æ–¥–µ–ª—å: {model}")
    print(f"üìù –ö–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {len(contexts)}")
    print(f"{'='*80}\n")
    
    results = []
    
    for i, ctx in enumerate(contexts, 1):
        print(f"\n{'‚îÄ'*80}")
        print(f"üìÑ –ö–æ–Ω—Ç–µ–∫—Å—Ç #{i} (ID: {ctx['id']}, –î–∞—Ç–∞: {ctx['message_date']})")
        print(f"{'‚îÄ'*80}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        preview = ctx['context_text'][:300] + "..." if len(ctx['context_text']) > 300 else ctx['context_text']
        print(f"\nüìñ –¢–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:\n{preview}\n")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ –º–æ–¥–µ–ª—å
        try:
            prompt = screening_prompt(ctx['context_text'])
            
            # o1 –º–æ–¥–µ–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç system messages –∏ response_format
            if model.startswith("o1"):
                # –î–ª—è o1 - —Ç–æ–ª—å–∫–æ user message
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
            elif model.startswith("gpt-5"):
                # GPT-5 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç temperature, top_p, logprobs
                # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç reasoning_effort –∏ verbosity
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∏–∑–Ω–µ—Å-–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π. –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–π –≤–∞–ª–∏–¥–Ω—ã–π JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    response_format={"type": "json_object"},
                    reasoning_effort="low",  # minimal | low | medium | high
                    # verbosity="medium"  # low | medium | high (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                )
            else:
                # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (gpt-4o, gpt-4o-mini)
                response = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": "–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–∏–∑–Ω–µ—Å-–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π. –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–π –≤–∞–ª–∏–¥–Ω—ã–π JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.3
                )
            
            result = json.loads(response.choices[0].message.content)
            
            # –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            print(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç:")
            print(f"   Has artifacts: {'‚úÖ –î–ê' if result.get('has_artifacts') else '‚ùå –ù–ï–¢'}")
            
            if result.get('has_artifacts'):
                types = result.get('artifact_types', [])
                print(f"   Types: {', '.join(types)}")
            
            print(f"   Confidence: {result.get('confidence', 0):.2f}")
            print(f"   Reasoning: {result.get('reasoning', 'N/A')}")
            
            # –¢–æ–∫–µ–Ω—ã
            usage = response.usage
            print(f"\nüí∞ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤:")
            print(f"   Input: {usage.prompt_tokens}")
            print(f"   Output: {usage.completion_tokens}")
            print(f"   Total: {usage.total_tokens}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            results.append({
                "context_id": ctx['id'],
                "has_artifacts": result.get('has_artifacts', False),
                "types": result.get('artifact_types', []),
                "confidence": result.get('confidence', 0),
                "tokens": usage.total_tokens
            })
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
            results.append({
                "context_id": ctx['id'],
                "error": str(e)
            })
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n\n{'='*80}")
    print(f"üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    print(f"{'='*80}\n")
    
    total = len(results)
    with_artifacts = sum(1 for r in results if r.get('has_artifacts'))
    total_tokens = sum(r.get('tokens', 0) for r in results)
    avg_confidence = sum(r.get('confidence', 0) for r in results) / total if total > 0 else 0
    
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤: {total}")
    print(f"üéØ –° –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏: {with_artifacts} ({with_artifacts/total*100:.1f}%)")
    print(f"üí∞ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}")
    print(f"üìà –°—Ä–µ–¥–Ω–∏–π confidence: {avg_confidence:.2f}")
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º
    all_types = []
    for r in results:
        all_types.extend(r.get('types', []))
    
    if all_types:
        print(f"\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤:")
        type_counts = {}
        for t in all_types:
            type_counts[t] = type_counts.get(t, 0) + 1
        
        for artifact_type, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   - {artifact_type}: {count}")
    
    # –û—Ü–µ–Ω–∫–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
    cost_per_1m = {
        "gpt-4o": (2.50, 10.00),           # input, output
        "gpt-4o-mini": (0.15, 0.60),
        "o1-preview": (15.00, 60.00),
        "o1-mini": (3.00, 12.00),
        "gpt-5": (5.00, 15.00),            # –ø—Ä–∏–º–µ—Ä–Ω–∞—è —Ü–µ–Ω–∞ –¥–ª—è gpt-5
    }
    
    if model in cost_per_1m:
        input_price, output_price = cost_per_1m[model]
        # –ü—Ä–∏–º–µ—Ä–Ω–æ 60/40 input/output
        input_cost = (total_tokens * 0.6) / 1_000_000 * input_price
        output_cost = (total_tokens * 0.4) / 1_000_000 * output_price
        total_cost = input_cost + output_cost
        print(f"\nüíµ –û—Ü–µ–Ω–æ—á–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost:.4f}")
    
    return results


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö")
    parser.add_argument("--account", default="ychukaev", help="Account name")
    parser.add_argument("--limit", type=int, default=10, help="Number of contexts to test")
    parser.add_argument("--model", default="gpt-4o", help="Model to use (gpt-5, gpt-4o, gpt-4o-mini, o1-preview, o1-mini)")
    
    args = parser.parse_args()
    
    # Load environment
    load_account_env(args.account)
    
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise SystemExit("OPENAI_API_KEY not found in environment")
    
    # Database path
    current_dir = Path(__file__).resolve().parent
    db_path = current_dir.parent.parent / "accounts" / args.account / "messages.sqlite"
    
    if not db_path.exists():
        raise SystemExit(f"Database not found: {db_path}")
    
    # Initialize OpenAI client
    client = OpenAI(api_key=api_key)
    
    # Get sample contexts
    print(f"üìö –ó–∞–≥—Ä—É–∂–∞—é {args.limit} —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ {db_path.name}...")
    contexts = get_sample_contexts(db_path, args.limit)
    
    if not contexts:
        raise SystemExit("No contexts found in database")
    
    # Test screening
    results = test_screening(client, contexts, model=args.model)
    
    # Save results
    output_path = Path(__file__).parent / "test_results.json"
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({
            "model": args.model,
            "contexts_tested": len(contexts),
            "results": results
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")


if __name__ == "__main__":
    main()

