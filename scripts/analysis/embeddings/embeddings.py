"""
–†–∞–±–æ—Ç–∞ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
"""
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from ..utils.gpt5_client import get_openai_client


def get_embedding(text: str, model: str = "text-embedding-3-small", client=None) -> Optional[List[float]]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ç–µ–∫—Å—Ç–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏ drill-down.
    
    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        model: –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —á–∏—Å–µ–ª (—ç–º–±–µ–¥–¥–∏–Ω–≥) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    if client is None:
        client = get_openai_client()
    
    try:
        response = client.embeddings.create(
            model=model,
            input=text[:8000]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è embeddings API
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"      ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        return None


def cosine_similarity_embedding(vec1: List[float], vec2: List[float]) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.
    
    Args:
        vec1: –ü–µ—Ä–≤—ã–π –≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        vec2: –í—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    
    Returns:
        –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ (–æ—Ç -1 –¥–æ 1)
    """
    try:
        import numpy as np
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
    except ImportError:
        # Fallback –±–µ–∑ numpy
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = sum(a * a for a in vec1) ** 0.5
        norm2 = sum(b * b for b in vec2) ** 0.5
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return dot_product / (norm1 * norm2)


def save_embeddings_for_level(
    level: str, 
    items: List[Dict[str, Any]], 
    output_dir: Path,
    client=None,
    cache_hours: Optional[float] = None
):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è.
    –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–æ–≤—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏.
    –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –µ—â–µ –Ω–µ—Ç –≤ —Ñ–∞–π–ª–µ.
    
    Args:
        level: 'raw_messages', 'compressed_chunks', 'summaries', 'tasks', 'projects'
        items: —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–æ–ª—è–º–∏ 'text', 'id', 'metadata' –∏ —Ç.–¥.
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
        cache_hours: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–µ—à–∞ –≤ —á–∞—Å–∞—Ö (None = –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
                     –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ –∏ —Ñ–∞–π–ª —Å—Ç–∞—Ä—à–µ - –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é
    """
    if client is None:
        client = get_openai_client()
    
    embeddings_file = output_dir / f"embeddings_{level}.json"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    existing_embeddings = {}
    if embeddings_file.exists():
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω cache_hours
        if cache_hours is not None:
            file_age_hours = (time.time() - embeddings_file.stat().st_mtime) / 3600
            if file_age_hours < cache_hours:
                print(f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —É—Ä–æ–≤–Ω—è '{level}' (–≤–æ–∑—Ä–∞—Å—Ç: {file_age_hours:.1f} —á–∞—Å–æ–≤)")
                print(f"   üíæ –§–∞–π–ª: {embeddings_file}")
                return
            else:
                print(f"   ‚ö† –§–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —É—Å—Ç–∞—Ä–µ–ª ({file_age_hours:.1f} —á–∞—Å–æ–≤), –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º...")
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            try:
                with open(embeddings_file, "r", encoding="utf-8") as f:
                    existing_data = json.load(f)
                    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å {id: embedding_data} –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                    for emb in existing_data:
                        emb_id = emb.get('id')
                        if emb_id is not None:
                            existing_embeddings[emb_id] = emb
                print(f"   üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(existing_embeddings)} —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
            except Exception as e:
                print(f"   ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                existing_embeddings = {}
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å (–Ω–æ–≤—ã–µ –∏–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ)
    items_to_process = []
    for item in items:
        item_id = item.get('id')
        text = item.get('text', '')
        if not text:
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —ç—Ç–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        if item_id is not None and item_id in existing_embeddings:
            # –≠–ª–µ–º–µ–Ω—Ç —É–∂–µ –µ—Å—Ç—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–∏–ª–∏ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ç–µ–∫—Å—Ç)
            continue
        
        items_to_process.append(item)
    
    if not items_to_process:
        print(f"   ‚úÖ –í—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —É—Ä–æ–≤–Ω—è '{level}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç ({len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤)")
        return
    
    print(f"   üìä –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(items_to_process)} –Ω–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∏–∑ {len(items)} –≤—Å–µ–≥–æ)...")
    
    new_embeddings_data = []
    for i, item in enumerate(items_to_process, 1):
        text = item.get('text', '')
        if not text:
            continue
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
        embedding = get_embedding(text, client=client)
        if embedding:
            new_embeddings_data.append({
                'id': item.get('id', len(existing_embeddings) + i),
                'text': text[:500],  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏
                'embedding': embedding,
                'metadata': item.get('metadata', {})
            })
            if i % 10 == 0:
                print(f"      –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(items_to_process)}...", end='\r', flush=True)
    
    print(f"      ‚úì –°–æ–∑–¥–∞–Ω–æ {len(new_embeddings_data)} –Ω–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏ –Ω–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    all_embeddings = list(existing_embeddings.values()) + new_embeddings_data
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    if all_embeddings:
        with open(embeddings_file, "w", encoding="utf-8") as f:
            json.dump(all_embeddings, f, ensure_ascii=False, indent=2)
        print(f"   üíæ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {embeddings_file} ({len(all_embeddings)} –≤—Å–µ–≥–æ, +{len(new_embeddings_data)} –Ω–æ–≤—ã—Ö)")


def find_relevant_sources_by_embedding(
    query_text: str,
    source_level: str,
    output_dir: Path,
    top_k: int = 5,
    similarity_threshold: float = 0.7,
    client=None
) -> List[Dict[str, Any]]:
    """
    –ù–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∏—Å—Ö–æ–¥–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è drill-down: –∫–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–∞–π—Ç–∏ –∏—Å—Ö–æ–¥–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∑–∞–¥–∞—á–∏/–ø—Ä–æ–µ–∫—Ç–∞.
    
    Args:
        query_text: —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏)
        source_level: —É—Ä–æ–≤–µ–Ω—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞ ('raw_messages', 'compressed_chunks', 'summaries')
        output_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Ñ–∞–π–ª–∞–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        similarity_threshold: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
        client: OpenAI –∫–ª–∏–µ–Ω—Ç (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)
    
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ö–æ–∂–µ—Å—Ç–∏
    """
    if client is None:
        client = get_openai_client()
    
    embeddings_file = output_dir / f"embeddings_{source_level}.json"
    
    if not embeddings_file.exists():
        print(f"   ‚ö† –§–∞–π–ª —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {embeddings_file}")
        return []
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    with open(embeddings_file, "r", encoding="utf-8") as f:
        source_embeddings = json.load(f)
    
    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
    query_embedding = get_embedding(query_text, client=client)
    if not query_embedding:
        return []
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å–æ –≤—Å–µ–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
    similarities = []
    for source in source_embeddings:
        similarity = cosine_similarity_embedding(query_embedding, source['embedding'])
        if similarity >= similarity_threshold:
            similarities.append({
                'id': source['id'],
                'text': source['text'],
                'similarity': similarity,
                'metadata': source.get('metadata', {})
            })
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ –±–µ—Ä–µ–º top_k
    similarities.sort(key=lambda x: x['similarity'], reverse=True)
    return similarities[:top_k]

