#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–ø–∏—Å–æ–∫ —Å –§–∞—Ä–º–∞+ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á
–ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–æ–¥—É–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
"""
import sqlite3
import json
import sys
from pathlib import Path
from typing import Dict

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ –º–æ–¥—É–ª–µ–π
# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
_script_dir = Path(__file__).resolve().parent
_project_root = _script_dir.parent.parent
if str(_project_root) not in sys.path:
    sys.path.insert(0, str(_project_root))

from scripts.analysis.utils.db import get_db_connection, get_all_messages_from_chats
from scripts.analysis.utils.formatting import format_messages_as_thread
from scripts.analysis.compression import compress_thread_with_smart_model
from scripts.analysis.embeddings import save_embeddings_for_level
from scripts.analysis.extraction import (
    extract_tasks_from_compressed_thread,
    extract_projects_with_drilldown,
    group_and_deduplicate_tasks
)
from scripts.analysis.utils.gpt5_client import get_openai_client


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent.parent  # tg-analyz/ (scripts/analysis/ -> scripts/ -> tg-analyz/)
DB_PATH = project_root / "accounts" / "ychukaev" / "messages.sqlite"

# ID —á–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
CHAT_IDS = {
    "–ï–≤–≥–µ–Ω–∏–π –ë–∞—Ç—Ä–∞–µ–≤": "5684787189",
    "–ù–∏–∫–∏—Ç–∞ –ë–∞–π–∫–∞–ª–æ–≤": "8109974557", 
    "IT farmaplus24.ru": "-1002823423591"
}

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
RESULTS_DIR = project_root / "results" / "farma"
COMPRESSED_PARTS_DIR = RESULTS_DIR / "compressed_parts"
EMBEDDINGS_DIR = RESULTS_DIR / "embeddings"
EXTRACTED_DIR = RESULTS_DIR / "extracted"
THREADS_DIR = RESULTS_DIR / "threads"


def main():
    print("üöÄ –ù–∞—á–∞–ª–æ —Ä–∞–±–æ—Ç—ã —Å–∫—Ä–∏–ø—Ç–∞...")
    print(f"üìÅ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: {DB_PATH}")
    print(f"   –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {DB_PATH.exists()}")
    
    if not DB_PATH.exists():
        print(f"‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)
    COMPRESSED_PARTS_DIR.mkdir(parents=True, exist_ok=True)
    EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)
    EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)
    THREADS_DIR.mkdir(parents=True, exist_ok=True)
    
    print("üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î...")
    conn = get_db_connection(DB_PATH)
    conn.row_factory = sqlite3.Row
    print("‚úì –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –ë–î")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∏–µ–Ω—Ç OpenAI
    client = get_openai_client()
    
    # –≠–¢–ê–ü 1: –°–±–æ—Ä –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —á–∞—Ç–æ–≤
    print(f"\n{'='*60}")
    print(f"üì• –≠–¢–ê–ü 1: –°–±–æ—Ä –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –∏–∑ —á–∞—Ç–æ–≤")
    print(f"{'='*60}")
    
    all_messages = get_all_messages_from_chats(conn, CHAT_IDS, limit_messages_per_chat=500)
    
    if not all_messages:
        print("‚ùå –°–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã!")
        conn.close()
        return
    
    # –≠–¢–ê–ü 2: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞
    print(f"\n{'='*60}")
    print(f"üìù –≠–¢–ê–ü 2: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ–¥–∏–Ω–æ–≥–æ –ø–æ—Ç–æ–∫–∞")
    print(f"{'='*60}")
    
    thread_text = format_messages_as_thread(all_messages)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ç–æ–∫
    raw_thread_file = THREADS_DIR / "farma_thread_raw.txt"
    with open(raw_thread_file, "w", encoding="utf-8") as f:
        f.write(thread_text)
    print(f"üíæ –ò—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ç–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {raw_thread_file} ({len(thread_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # –≠–¢–ê–ü 3: –°–∂–∞—Ç–∏–µ –¥–∏–∞–ª–æ–≥–∞ –¥–æ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
    print(f"\n{'='*60}")
    print(f"üß† –≠–¢–ê–ü 3: –°–∂–∞—Ç–∏–µ –¥–∏–∞–ª–æ–≥–∞ –¥–æ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤")
    print(f"{'='*60}")
    
    compressed_text = compress_thread_with_smart_model(
        thread_text,
        max_chunk_size=10000,
        output_dir=COMPRESSED_PARTS_DIR,
        client=client
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∂–∞—Ç—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
    compressed_file = THREADS_DIR / "farma_thread_compressed.txt"
    with open(compressed_file, "w", encoding="utf-8") as f:
        f.write(compressed_text)
    print(f"üíæ –°–∂–∞—Ç—ã–π –ø–æ—Ç–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {compressed_file} ({len(compressed_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    # –≠–¢–ê–ü 3.5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è drill-down
    print(f"\n{'='*60}")
    print(f"üìä –≠–¢–ê–ü 3.5: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è drill-down")
    print(f"{'='*60}")
    
    print(f"\n   üìä –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∏—Å—Ö–æ–¥–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π...")
    raw_messages_for_embeddings = [
        {
            'id': msg.get('message_id', i),
            'text': f"{msg.get('sender_name', '')}: {msg.get('content', '')}",
            'metadata': {
                'message_id': msg.get('message_id'),
                'date': msg.get('date'),
                'chat_id': msg.get('chat_id'),
                'chat_name': msg.get('chat_name')
            }
        }
        for i, msg in enumerate(all_messages)
    ]
    save_embeddings_for_level('raw_messages', raw_messages_for_embeddings, COMPRESSED_PARTS_DIR, client)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤—ã–∂–∏–º–∫–∏
    print(f"\n   üìä –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –≤—ã–∂–∏–º–∫–∏...")
    save_embeddings_for_level('summaries', [{
        'id': 'final_summary',
        'text': compressed_text,
        'metadata': {'type': 'sliding_window_summary'}
    }], COMPRESSED_PARTS_DIR, client)
    
    # –≠–¢–ê–ü 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á —Å –≤–µ—Ç–∫–∞–º–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–π
    print(f"\n{'='*60}")
    print(f"üìã –≠–¢–ê–ü 4: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–¥–∞—á —Å –≤–µ—Ç–∫–∞–º–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–π")
    print(f"{'='*60}")
    
    tasks_result = extract_tasks_from_compressed_thread(compressed_text, client)
    all_tasks = tasks_result.get("tasks", [])
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—ã—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    raw_output_file = EXTRACTED_DIR / "farma_tasks_extracted_raw.json"
    with open(raw_output_file, "w", encoding="utf-8") as f:
        json.dump(all_tasks, f, ensure_ascii=False, indent=2)
    
    print(f"\n{'='*60}")
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"   –í—Å–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–æ –∑–∞–¥–∞—á: {len(all_tasks)}")
    print(f"üíæ –°—ã—Ä—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {raw_output_file}")
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏ –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏
    if all_tasks:
        print(f"\n{'='*60}")
        print(f"üîó –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∏ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∑–∞–¥–∞—á")
        print(f"{'='*60}")
        
        grouped_result = group_and_deduplicate_tasks(all_tasks, client=client)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        grouped_output_file = EXTRACTED_DIR / "farma_tasks_extracted.json"
        with open(grouped_output_file, "w", encoding="utf-8") as f:
            json.dump(grouped_result, f, ensure_ascii=False, indent=2)
        print(f"üíæ –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {grouped_output_file}")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {grouped_result['total_tasks']}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á: {len(grouped_result['unique_tasks'])}")
        print(f"   –ì—Ä—É–ø–ø –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(grouped_result['duplicate_groups'])}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–¥–∞—á
        print(f"\n   üìä –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∑–∞–¥–∞—á...")
        tasks_for_embeddings = [
            {
                'id': i,
                'text': f"{task.get('title', '')} {task.get('description', '')}",
                'metadata': task
            }
            for i, task in enumerate(all_tasks)
        ]
        save_embeddings_for_level('tasks', tasks_for_embeddings, COMPRESSED_PARTS_DIR, client)
    
    # –≠–¢–ê–ü 5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å drill-down
    print(f"\n{'='*60}")
    print(f"üìä –≠–¢–ê–ü 5: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å drill-down")
    print(f"{'='*60}")
    
    projects_result = extract_projects_with_drilldown(
        compressed_text,
        conn,
        COMPRESSED_PARTS_DIR,
        CHAT_IDS,
        use_drilldown=True,
        client=client
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–µ–∫—Ç—ã
    projects_file = EXTRACTED_DIR / "farma_projects_extracted.json"
    with open(projects_file, "w", encoding="utf-8") as f:
        json.dump(projects_result, f, ensure_ascii=False, indent=2)
    print(f"üíæ –ü—Ä–æ–µ–∫—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {projects_file}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ–≤
    projects = projects_result.get("projects", [])
    if projects:
        print(f"\n   üìä –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–æ–≤...")
        projects_for_embeddings = [
            {
                'id': i,
                'text': f"{project.get('name', '')} {project.get('description', '')}",
                'metadata': project
            }
            for i, project in enumerate(projects)
        ]
        save_embeddings_for_level('projects', projects_for_embeddings, COMPRESSED_PARTS_DIR, client)
    
    conn.close()
    print(f"\n‚úÖ –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


if __name__ == "__main__":
    main()

