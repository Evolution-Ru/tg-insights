#!/usr/bin/env python3
"""
–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–∞—Ç—á–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–µ—à
"""
import json
import os
from dotenv import load_dotenv
from pathlib import Path
from openai import OpenAI

# –ó–∞–≥—Ä—É–∂–∞–µ–º .env
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent.parent  # tg-analyz/
account_env_path = project_root / "data" / "accounts" / "ychukaev" / ".env"

if account_env_path.exists():
    load_dotenv(account_env_path, override=True)

load_dotenv(project_root / ".env")

api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω")

client = OpenAI(api_key=api_key, timeout=600.0)

# ID –±–∞—Ç—á–∞ (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∫–∞–∫ –∞—Ä–≥—É–º–µ–Ω—Ç –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏)
import sys
batch_id = sys.argv[1] if len(sys.argv) > 1 else "batch_691064ceb7088190a65faf2142f5458d"

# –ü—É—Ç—å –∫ –∫–µ—à—É
cache_dir = project_root / "results" / "farma" / "compressed_parts" / "cache"
cache_dir.mkdir(parents=True, exist_ok=True)

print(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–∞—Ç—á–∞ {batch_id}...\n")

# –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ –±–∞—Ç—á–∞
batch_detail = client.batches.retrieve(batch_id)
if batch_detail.status != "completed" or not batch_detail.output_file_id:
    print(f"‚ùå –ë–∞—Ç—á –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω –∏–ª–∏ –Ω–µ—Ç output —Ñ–∞–π–ª–∞")
    exit(1)

# –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
output_file = client.files.content(batch_detail.output_file_id)
output_content = output_file.read().decode('utf-8')

# –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
results_data = []
for line in output_content.strip().split('\n'):
    if line.strip():
        result = json.loads(line)
        results_data.append(result)

print(f"‚úì –ü–æ–ª—É—á–µ–Ω–æ {len(results_data)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n")

# –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
processed_count = 0
for result_data in results_data:
    custom_id = result_data.get('custom_id', '')
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º hash –∏–∑ custom_id (—Ñ–æ—Ä–º–∞—Ç: chunk_{idx}_{hash})
    parts = custom_id.split('_')
    if len(parts) < 3:
        print(f"‚ö† –ü—Ä–æ–ø—É—â–µ–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–µ–≤–µ—Ä–Ω—ã–º custom_id: {custom_id}")
        continue
    
    chunk_hash = parts[2]
    cache_file = cache_dir / f"{chunk_hash}.txt"
    
    # –ï—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å –≤ –∫–µ—à–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if cache_file.exists():
        print(f"‚úì –£–∂–µ –≤ –∫–µ—à–µ: {chunk_hash}")
        continue
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º compressed —Ç–µ–∫—Å—Ç –∏–∑ –æ—Ç–≤–µ—Ç–∞
    response_body = result_data.get('response', {}).get('body', {})
    compressed = ""
    
    # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç responses API
    if 'output_text' in response_body:
        compressed = response_body['output_text']
    elif 'output' in response_body:
        output = response_body['output']
        if isinstance(output, str):
            compressed = output
        elif isinstance(output, list):
            chunks = []
            for item in output:
                if isinstance(item, dict):
                    if 'text' in item:
                        chunks.append(item['text'])
                    elif 'content' in item:
                        for c in item.get('content', []):
                            if isinstance(c, dict) and 'text' in c:
                                chunks.append(c['text'])
            compressed = '\n'.join(chunks)
    
    if compressed:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
        with open(cache_file, 'w', encoding='utf-8') as f:
            f.write(compressed.strip())
        print(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –∫–µ—à: {chunk_hash} ({len(compressed)} —Å–∏–º–≤–æ–ª–æ–≤)")
        processed_count += 1
    else:
        print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –¥–ª—è {chunk_hash}")

print(f"\n‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –Ω–æ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
print(f"üìÅ –ö–µ—à: {cache_dir}")

# –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –±–∞—Ç—á–∞
metadata_file = cache_dir.parent / "batch_metadata.json"
batch_metadata = {
    "batch_id": batch_id,
    "created_at": batch_detail.created_at,
    "created_at_iso": None,  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é timestamp
    "status": "completed",
    "completed_at": batch_detail.completed_at,
    "completed_at_iso": None,
    "input_file_id": batch_detail.input_file_id if hasattr(batch_detail, 'input_file_id') else None,
    "output_file_id": batch_detail.output_file_id,
    "total_chunks": len(results_data),
    "processed_chunks": processed_count
}

batch_metadata_list = []
if metadata_file.exists():
    with open(metadata_file, 'r', encoding='utf-8') as f:
        batch_metadata_list = json.load(f)

# –û–±–Ω–æ–≤–ª—è–µ–º –∏–ª–∏ –¥–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
found = False
for bm in batch_metadata_list:
    if bm.get("batch_id") == batch_id:
        bm.update(batch_metadata)
        found = True
        break

if not found:
    batch_metadata_list.append(batch_metadata)

with open(metadata_file, 'w', encoding='utf-8') as f:
    json.dump(batch_metadata_list, f, ensure_ascii=False, indent=2)

print(f"üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {metadata_file}")

